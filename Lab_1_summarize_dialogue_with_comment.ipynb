{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_E5jMG5XeYB"
      },
      "source": [
        "# Generative AI Use Case: Summarize Dialogue\n",
        "\n",
        "Welcome to the practical side of this course. In this lab you will do the dialogue summarization task using generative AI. You will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task you need. By comparing zero shot, one shot, and few shot inferences, you will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpAICV6IXeYC"
      },
      "source": [
        "# Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmdtTpK2XeYD"
      },
      "source": [
        "- [ 1 - Set up Kernel and Required Dependencies](#1)\n",
        "- [ 2 - Summarize Dialogue without Prompt Engineering](#2)\n",
        "- [ 3 - Summarize Dialogue with an Instruction Prompt](#3)\n",
        "  - [ 3.1 - Zero Shot Inference with an Instruction Prompt](#3.1)\n",
        "  - [ 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5](#3.2)\n",
        "- [ 4 - Summarize Dialogue with One Shot and Few Shot Inference](#4)\n",
        "  - [ 4.1 - One Shot Inference](#4.1)\n",
        "  - [ 4.2 - Few Shot Inference](#4.2)\n",
        "- [ 5 - Generative Configuration Parameters for Inference](#5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7BFZwC6XeYD"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Set up Kernel and Required Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "-HtOdNvvXeYD"
      },
      "source": [
        "First, check that the correct kernel is chosen.\n",
        "\n",
        "<img src=\"images/kernel_set_up.png\" width=\"300\"/>\n",
        "\n",
        "The following code checks the compute instance type to ensure there are enough compute resources to run this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W9axdN-uXeYD",
        "outputId": "aa66a79f-e4a0-45e6-ad03-e04e197726d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instance verified ✓\n"
          ]
        }
      ],
      "source": [
        "def verify_m5_2xlarge():\n",
        "    import subprocess, psutil\n",
        "    c,m=int(subprocess.getoutput(\"nproc\")),psutil.virtual_memory().total/1024**3\n",
        "    assert c==8 and abs(m-32)<2,f\"ERROR: Wrong instance type. Select ml.m5.2xlarge (8 vCPUs, 32 GiB). Current: {c} vCPUs, {m:.1f} GiB\\nFix: File->Shut Down, then select ml.m5.2xlarge\"\n",
        "    print(\"Instance verified ✓\")\n",
        "verify_m5_2xlarge()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "eJA3rmj9XeYE"
      },
      "source": [
        "Now install the required packages to use PyTorch and Hugging Face transformers and datasets.\n",
        "\n",
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "g-LZFlY9XeYE",
        "outputId": "3449b8f3-5394-4d4a-a5eb-4bfbfd3bc013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (25.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /opt/conda/lib/python3.12/site-packages (2.18.0)\n",
            "Requirement already satisfied: keras==3.9.0 in /opt/conda/lib/python3.12/site-packages (3.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (5.28.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (75.8.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.18.0) (0.4.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from keras==3.9.0) (1.26.4)\n",
            "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras==3.9.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras==3.9.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras==3.9.0) (0.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras==3.9.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras==3.9.0) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras==3.9.0) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
            "jupyter-ai 2.30.0 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
            "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
            "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.8 which is incompatible.\n",
            "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.16 which is incompatible.\n",
            "s3fs 2024.10.0 requires fsspec==2024.10.0.*, but you have fsspec 2023.10.0 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# First upgrade pip\n",
        "%pip install --upgrade pip\n",
        "\n",
        "# Install tensorflow and keras first\n",
        "%pip install tensorflow==2.18.0 keras==3.9.0\n",
        "\n",
        "# Install torch and torchdata\n",
        "%pip install --no-deps torch==2.5.1 torchdata==0.6.0 --quiet\n",
        "\n",
        "# Then install other packages except TRL\n",
        "%pip install -U \\\n",
        "    datasets==2.17.0 \\\n",
        "    transformers==4.38.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    peft==0.3.0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "pXtVwwcLXeYF"
      },
      "source": [
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "3lYf7VE5XeYF"
      },
      "source": [
        "Load the datasets, Large Language Model (LLM), tokenizer, and configurator. Do not worry if you do not understand yet all of those components - they will be described and discussed later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "UdW5bEuPXeYF"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# DATA LOADER\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from datasets import load_dataset       # Hugging Face helper that grabs a\n",
        "                                        # dataset (local file, URL, or HF Hub)\n",
        "                                        # and turns it into a fast, memory-\n",
        "                                        # mapped object you can iterate over\n",
        "                                        # in mini-batches.  One-liner for\n",
        "                                        # reading MIMIC-IV CSV, JSON, Parquet,\n",
        "                                        # etc.\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# MODEL CLASS: sequence-to-sequence (T5, FLAN-T5, BART, …)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from transformers import AutoModelForSeq2SeqLM   # “Auto” picks the right\n",
        "                                                 # concrete class once you\n",
        "                                                 # pass model_name (e.g.,\n",
        "                                                 # \"google/flan-t5-base\").\n",
        "                                                 # All encoder-decoder models\n",
        "                                                 # that generate **new text**\n",
        "                                                 # (summaries, translations)\n",
        "                                                 # fall under Seq2SeqLM.\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# TOKENIZER\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from transformers import AutoTokenizer          # Converts raw strings ↔︎ lists\n",
        "                                                # of integer IDs.  Must be the\n",
        "                                                # *exact* tokenizer that matches\n",
        "                                                # the model checkpoint; the\n",
        "                                                # Auto* helper ensures that.\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# GENERATION SETTINGS\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from transformers import GenerationConfig       # A tidy container for all the\n",
        "                                                # decoding knobs: max_new_tokens,\n",
        "                                                # temperature, top_k / top_p,\n",
        "                                                # penalty_alpha (contrastive\n",
        "                                                # decoding), etc.  You can save\n",
        "                                                # a config once and reuse it\n",
        "                                                # across multiple generate()\n",
        "                                                # calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXAQ6-uUXeYF"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Summarize Dialogue without Prompt Engineering\n",
        "\n",
        "In this use case, you will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "Let's upload some simple dialogues from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "5f37c6e3705f4a848936e81b775429f4",
            "570b47f2ba31481487d14f92484e2c2a",
            "e74a6be77dfa4bde82e79542ac6d0a63",
            "3d3ec36f1ddb4c12b47b5195a93b0843",
            "f29f65afc4bf4588a06600f95eab11c2",
            "0e6ea981e14047a1a4d0322d711b0550",
            "d366efb097b74e2f96f5a1b936794c8d"
          ]
        },
        "id": "EnODeaEjXeYF",
        "outputId": "3ab0a882-4c41-4d8a-a598-35e018f59820"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f37c6e3705f4a848936e81b775429f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "570b47f2ba31481487d14f92484e2c2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e74a6be77dfa4bde82e79542ac6d0a63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d3ec36f1ddb4c12b47b5195a93b0843",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f29f65afc4bf4588a06600f95eab11c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
            "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e6ea981e14047a1a4d0322d711b0550",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
            "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d366efb097b74e2f96f5a1b936794c8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
            "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 1) Pick a dataset that already lives on the Hugging Face Hub.\n",
        "#    \"knkarthick/dialogsum\" is a public repo containing a benchmark\n",
        "#    of short human-written dialogues + reference summaries.\n",
        "#    (Perfect sandbox before you jump to the larger MIMIC-IV notes.)\n",
        "# ------------------------------------------------------------------\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"   # <hub_owner>/<repo_name>\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2) Download (or read from cache) and return a DatasetDict object.\n",
        "#    • If this is your first run, the files are pulled over HTTP,\n",
        "#      decompressed, then stored under ~/.cache/huggingface/datasets.\n",
        "#    • Next time, load_dataset() will grab them locally—no re-download.\n",
        "#\n",
        "#    Result structure:\n",
        "#      dataset[\"train\"]   →  tens of thousands of examples\n",
        "#      dataset[\"validation\"]\n",
        "#      dataset[\"test\"]\n",
        "#\n",
        "#    Each split behaves like a Python list of dicts, but with vectorized\n",
        "#    methods such as .map(), .shuffle(), .select(), .train_test_split().\n",
        "# ------------------------------------------------------------------\n",
        "dataset = load_dataset(huggingface_dataset_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "gz-On3V2XeYF"
      },
      "source": [
        "Print a couple of dialogues with their baseline summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6N0jYd5rXeYF",
        "outputId": "9cf8f143-c56e-48ee-fa67-ed2c174e0a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT DIALOGUE:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT DIALOGUE:\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1) Pick which rows you want to print from the test set.\n",
        "#    • Each row in dataset[\"test\"] is one dialogue-summary pair.\n",
        "#    • Row 0 = the very first example, row 1 = the next, etc.\n",
        "#    • Here we choose rows 40 and 200 just to show variety.\n",
        "# ------------------------------------------------------------\n",
        "example_rows = [40, 200]     # <- change to [0, 1] to see the first two rows\n",
        "                             #    or [5] to see only row 5, etc.\n",
        "\n",
        "# ──────────────────────────────────────────────────────────\n",
        "# 2) Build a long dashed line \"----------…\"\n",
        "#    • The generator ('' for x in range(100)) yields 100 empty\n",
        "#      strings.  Joining them with '-' puts a dash between each\n",
        "#      empty string, giving you 99 dashes total.\n",
        "#    • A simpler way would be '-' * 100, but the lab author\n",
        "#      chose .join(); we keep it and explain the trick.\n",
        "# ──────────────────────────────────────────────────────────\n",
        "dash_line = '-'.join('' for _ in range(100))   # visual separator\n",
        "\n",
        "# ──────────────────────────────────────────────────────────\n",
        "# 3) Loop over our chosen indices and print a nicely-formatted\n",
        "#    preview so you can eyeball what the raw dialogue looks like\n",
        "#    and compare it to the human-written reference summary.\n",
        "# ──────────────────────────────────────────────────────────\n",
        "for i, idx in enumerate(example_indices):\n",
        "\n",
        "    # Header line\n",
        "    print(dash_line)\n",
        "    print(f'Example {i + 1}')        # friendly counter: 1, 2, …\n",
        "\n",
        "    # Another separator for readability\n",
        "    print(dash_line)\n",
        "\n",
        "    # ---- Show the full dialogue --------------------------------\n",
        "    print('INPUT DIALOGUE:')\n",
        "    print(dataset['test'][idx]['dialogue'])    # raw multi-turn convo\n",
        "    print(dash_line)\n",
        "\n",
        "    # ---- Show the gold-standard summary ------------------------\n",
        "    print('BASELINE HUMAN SUMMARY:')\n",
        "    print(dataset['test'][idx]['summary'])     # written by annotators\n",
        "    print(dash_line)\n",
        "\n",
        "    # Blank line between examples\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga9wK_evXeYG"
      },
      "source": [
        "Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAYlS40Z3l-v",
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "9b8eb5ea08114dec9ffce63acc9ead6a",
            "b842b2a17b0946499681cba0fbe58b9d",
            "e258817298604f279d2ae043ece0cc2d"
          ]
        },
        "outputId": "ae992ebb-8bc2-4f4f-9862-6f05cdb96fdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b8eb5ea08114dec9ffce63acc9ead6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b842b2a17b0946499681cba0fbe58b9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e258817298604f279d2ae043ece0cc2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1) Pick which checkpoint (i.e., “pre-trained model file”) you\n",
        "#    want from the Hugging Face Hub.\n",
        "#    • \"google/flan-t5-base\" = 580 M-parameter FLAN-T5, already\n",
        "#      fine-tuned by Google to follow instructions in plain English.\n",
        "#      (Good balance of quality vs. GPU/CPU memory.)\n",
        "# ------------------------------------------------------------\n",
        "model_name = \"google/flan-t5-base\"   # try \"google/flan-t5-large\" for higher quality,\n",
        "                                     # or \"t5-small\" if you have very little RAM.\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Download the weights + configuration (or read them from\n",
        "#    ~/.cache if you’ve done it before) and build a PyTorch model\n",
        "#    object that’s ready for inference.\n",
        "#    • AutoModelForSeq2SeqLM   → “Auto” figures out the exact class\n",
        "#      (here, T5ForConditionalGeneration) based on the checkpoint.\n",
        "#    • .from_pretrained(...)   → handles:\n",
        "#         – fetching files from the Hub\n",
        "#         – reading config.json (d_model, n_layers, etc.)\n",
        "#         – loading the weight tensors into the architecture\n",
        "# ------------------------------------------------------------\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# (Optional) move to GPU if available\n",
        "# model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPqQA3TT3l_I",
        "tags": []
      },
      "source": [
        "To perform encoding and decoding, you need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models.\n",
        "\n",
        "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "3ebcb780509c45e1af689972829782cd",
            "39b72f6898c84b099c5305935af030dc",
            "51c5a66f95154f9c8ae1c163123677da",
            "f36766a5116d44f8bd9f44aaf61c544b"
          ]
        },
        "id": "FnmLPPw8XeYG",
        "outputId": "d6cec3c5-9d9f-4575-86ce-2ca064b5053e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ebcb780509c45e1af689972829782cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39b72f6898c84b099c5305935af030dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51c5a66f95154f9c8ae1c163123677da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f36766a5116d44f8bd9f44aaf61c544b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# TOKENIZER: turns raw text  ←→  integer IDs that the model expects\n",
        "#\n",
        "# • AutoTokenizer            → picks the correct tokenizer class\n",
        "#                              (T5Tokenizer, LlamaTokenizer, etc.)\n",
        "#   based solely on the same\n",
        "#   `model_name` we used for the weights.\n",
        "#\n",
        "# • .from_pretrained(...)    → downloads (or caches) the tokenizer\n",
        "#                              files: vocab.json, merges.txt, special\n",
        "#                              tokens, normalisation rules, etc.\n",
        "#\n",
        "# • use_fast=True            → prefer the “fast” Rust-based tokenizer\n",
        "#                              (tokenizers library) which is:\n",
        "#                                – 5-10× quicker than the pure-Python version\n",
        "#                                – functionally identical for most models\n",
        "#                              Falls back to the slow version if a fast\n",
        "#                              implementation doesn’t exist for this checkpoint.\n",
        "# ------------------------------------------------------------\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VcWkJOCEXeYG"
      },
      "source": [
        "Test the tokenizer encoding and decoding a simple sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kBdgN7IoXeYG",
        "outputId": "1e792a8a-cd4f-48ba-b159-fc89eb49b495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENCODED SENTENCE:\n",
            "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
            "\n",
            "DECODED SENTENCE:\n",
            "What time is it, Tom?\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1) Raw text we want to feed into (or recover from) the model.\n",
        "#    Feel free to change this string to anything you like.\n",
        "# ------------------------------------------------------------\n",
        "sentence = \"What time is it, Tom?\"\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) ENCODE  (text → integer IDs)\n",
        "#    • tokenizer(sentence, return_tensors='pt')\n",
        "#        – Splits the text into sub-word pieces\n",
        "#          (e.g., \"What\", \"▁time\", \"▁is\", …).\n",
        "#        – Maps each piece to its numeric ID from the model’s\n",
        "#          vocabulary file.\n",
        "#        – Adds special tokens if needed (e.g., <pad>, </s>).\n",
        "#        – return_tensors='pt'  → wrap the result in PyTorch\n",
        "#          tensors so it can go straight into the model.\n",
        "#\n",
        "#    The output is a *dict* like:\n",
        "#        {\n",
        "#          \"input_ids\":      tensor([[ 262,  3635,  19,  34,    6,  924,    58 ]]),\n",
        "#          \"attention_mask\": tensor([[   1,    1,   1,   1,    1,    1,     1 ]])\n",
        "#        }\n",
        "# ------------------------------------------------------------\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) DECODE  (IDs → text, mainly for sanity-checking)\n",
        "#    • tokenizer.decode(ids, skip_special_tokens=True)\n",
        "#        – Converts the list of IDs back to readable tokens.\n",
        "#        – skip_special_tokens=True  drops things like <pad> or </s>\n",
        "#          so you see only the original words.\n",
        "#\n",
        "#    We grab the first (and only) row with [0] because\n",
        "#    sentence_encoded[\"input_ids\"] has shape [batch, seq_len].\n",
        "# ------------------------------------------------------------\n",
        "sentence_decoded = tokenizer.decode(\n",
        "    sentence_encoded[\"input_ids\"][0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Print both versions so you can verify round-tripping works.\n",
        "# ------------------------------------------------------------\n",
        "print('ENCODED SENTENCE (token IDs):')\n",
        "print(sentence_encoded[\"input_ids\"][0])\n",
        "\n",
        "print('\\nDECODED SENTENCE (back to text):')\n",
        "print(sentence_decoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vZN1uJQXeYG"
      },
      "source": [
        "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GI7h3WNRXeYG",
        "outputId": "6b4066fa-2602-4837-86f4-4fa69cf11d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "Person1: It's ten to nine.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
            "#Person1#: I'm thinking of upgrading my computer.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# ITERATE THROUGH the sample indices and have the model\n",
        "# produce a summary for each dialogue.\n",
        "#\n",
        "# ─ Variables used ────────────────────────────────────────────────\n",
        "# example_indices : list of row numbers we picked earlier\n",
        "# dataset         : Hugging Face DatasetDict already loaded\n",
        "# tokenizer       : converts text ↔ token IDs\n",
        "# model           : FLAN-T5 network we loaded\n",
        "# dash_line       : a long \"------\" string for nicer console output\n",
        "# ------------------------------------------------------------------\n",
        "for i, index in enumerate(example_indices):\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 1) FETCH raw data from the test split\n",
        "    #    • Each item is a dict with keys: \"dialogue\", \"summary\", …\n",
        "    # --------------------------------------------------------------\n",
        "    dialogue = dataset['test'][index]['dialogue'] # full conversation\n",
        "    summary = dataset['test'][index]['summary'] # human summary\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 2) ENCODE the dialogue so the model can understand it\n",
        "    #    return_tensors='pt'  → get PyTorch tensors (not lists)\n",
        "    # --------------------------------------------------------------\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 3) GENERATE a summary with the model\n",
        "    #    • model.generate(ids, max_new_tokens=50)\n",
        "    #        – takes the encoded input IDs\n",
        "    #        – autoregressively produces up to 50 new tokens\n",
        "    #    • The output is a tensor of token IDs → decode back to text\n",
        "    # --------------------------------------------------------------\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"], # prompt IDs\n",
        "            max_new_tokens=50, # cap length of generated summary\n",
        "        )[0],  # [0] because batch-size is 1\n",
        "        skip_special_tokens=True # drop <pad>, </s>, etc.\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 4) PRINT everything in a readable block\n",
        "    # --------------------------------------------------------------\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PotRE5WTXeYG"
      },
      "source": [
        "You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Seems it just makes up the next sentence in the dialogue. Prompt engineering can help here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aNoWFbTXeYG"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Summarize Dialogue with an Instruction Prompt\n",
        "\n",
        "Prompt engineering is an important concept in using foundation models for text generation. You can check out [this blog](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) from Amazon Science for a quick introduction to prompt engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyzEgXLcXeYG"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Zero Shot Inference with an Instruction Prompt\n",
        "\n",
        "In order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called **zero shot inference**.  You can check out [this blog from AWS](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/) for a quick description of what zero shot learning is and why it is an important concept to the LLM model.\n",
        "\n",
        "Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WQn8qKwXXeYG",
        "outputId": "e5fcb141-6a0e-4000-db18-4e12ffb36952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Summarize the following conversation.\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "Summary:\n",
            "    \n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "The train is about to leave.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Summarize the following conversation.\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Summary:\n",
            "    \n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "#Person1#: I'm thinking of upgrading my computer.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# ZERO-SHOT PROMPT ENGINEERING EXAMPLE\n",
        "#   • Same idea as the previous cell, *but* we wrap the dialogue\n",
        "#     inside an explicit instruction:  “Summarize the following…”.\n",
        "#   • That single natural-language instruction often makes FLAN-T5\n",
        "#     produce much cleaner summaries (called “zero-shot” because we\n",
        "#     give *no* example summaries—just the task description).\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    # --------------------------------------------------------------\n",
        "    # 1) Pull the raw dialogue + its human summary from the test set\n",
        "    # --------------------------------------------------------------\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 2) Build an instruction-style prompt\n",
        "    #    • f\"\"\" … \"\"\"  is a multi-line f-string: we can insert the\n",
        "    #      dialogue variable right inside the string.\n",
        "    #    • The trailing word “Summary:” nudges the model to start\n",
        "    #      writing immediately after that label.\n",
        "    # --------------------------------------------------------------\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 3) ENCODE the prompt and GENERATE up to 50 new tokens\n",
        "    # --------------------------------------------------------------\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"], # encoded prompt\n",
        "            max_new_tokens=50, # stop after 50 generated tokens\n",
        "        )[0],  # batch size = 1 → grab row 0\n",
        "        skip_special_tokens=True # remove <pad>, </s>, etc.\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 4) Pretty-print everything side by side for inspection\n",
        "    # --------------------------------------------------------------\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61In--duXeYH"
      },
      "source": [
        "This is much better! But the model still does not pick up on the nuance of the conversations though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6KIOi2eXeYH"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "- Experiment with the `prompt` text and see how the inferences will be changed. Will the inferences change if you end the prompt with just empty string vs. `Summary: `?\n",
        "- Try to rephrase the beginning of the `prompt` text from `Summarize the following conversation.` to something different - and see how it will influence the generated output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XgLxT1XeYH"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5\n",
        "\n",
        "Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, you will use one of the [pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5zJmFQMJXeYH",
        "outputId": "61e92937-8f9d-4f5f-ff8f-13cdd0d0f74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "What was going on?\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "Tom is late for the train.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "What was going on?\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# ZERO-SHOT, *CONVERSATIONAL* PROMPT\n",
        "#   • Same goal (get the model to summarise) but phrased as a casual\n",
        "#     question:  “What was going on?”  Sometimes open-ended wording\n",
        "#     elicits a more narrative answer than the explicit “Summarize…”.\n",
        "# ------------------------------------------------------------------\n",
        "for i, index in enumerate(example_indices):\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 1) Fetch the dialogue text + its human-written summary\n",
        "    # --------------------------------------------------------------\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 2) Build an *informal* prompt\n",
        "    #    • Section label “Dialogue:” keeps the raw text readable.\n",
        "    #    • Followed by an open question: “What was going on?”\n",
        "    # --------------------------------------------------------------\n",
        "    prompt = f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "\"\"\"\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 3) Tokenise the prompt and generate up to 50 tokens\n",
        "    # --------------------------------------------------------------\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW-HCSnqXeYH"
      },
      "source": [
        "Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what you will try to solve with the few shot inferencing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5vNmsKXXeYH"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Summarize Dialogue with One Shot and Few Shot Inference\n",
        "\n",
        "**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task.  You can read more about it in [this blog from HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "AsHzTp1nXeYH"
      },
      "source": [
        "<a name='4.1'></a>\n",
        "### 4.1 - One Shot Inference\n",
        "\n",
        "Let's build a function that takes a list of `example_indices_full`, generates a prompt with full examples, then at the end appends the prompt which you want the model to complete (`example_index_to_summarize`).  You will use the same FLAN-T5 prompt template from section [3.2](#3.2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1I36jkJDXeYH"
      },
      "outputs": [],
      "source": [
        "## ------------------------------------------------------------------\n",
        "# FEW-SHOT PROMPT CONSTRUCTOR\n",
        "#   • Purpose: build *one big string* that contains N “demo” pairs\n",
        "#     (dialogue + correct summary) **plus** one final dialogue the\n",
        "#     model must summarise itself.\n",
        "#   • This is called *few-shot prompting*: we show the model examples\n",
        "#     of the task so it can mimic the pattern.\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "   \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    example_indices_full : list[int]\n",
        "        Dataset rows we will use as *demonstrations*.  Each one adds:\n",
        "            Dialogue\n",
        "            + question “What was going on?”\n",
        "            + the *answer* (human summary)\n",
        "\n",
        "    example_index_to_summarize : int\n",
        "        Row that will *not* include an answer.  The model will have to\n",
        "        generate the summary for this final dialogue.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    prompt : str\n",
        "        A single, multi-line prompt ready to feed into the tokenizer.\n",
        "        Format:\n",
        "            Dialogue\n",
        "            Question\n",
        "            Human summary\n",
        "            <blank line>\n",
        "        … repeated for each demo, and finally\n",
        "            Dialogue\n",
        "            Question\n",
        "            ←-- model continues here\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = '' # start with an empty string\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # 1) Add *demonstration* examples (dialogue + known summary)\n",
        "    # --------------------------------------------------------------\n",
        "    for index in example_indices_full:\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
        "        # The triple line-break after {summary} acts as a *stop\n",
        "        # sequence* for FLAN-T5.  It separates one QA pair from the\n",
        "        # next so the model learns the boundary.\n",
        "        prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "{summary}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    # --------------------------------------------------------------\n",
        "    # 2) Append the *new* dialogue that needs summarising\n",
        "    #    (no answer provided — model must fill this in)\n",
        "    # --------------------------------------------------------------\n",
        "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "\"\"\" # <-- we stop after the question; model will continue here\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "pD6OAIpuXeYH"
      },
      "source": [
        "Construct the prompt to perform one shot inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8za9Lz11XeYH",
        "outputId": "ed721df6-3dff-4733-a404-00e7eaf1ed8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "What was going on?\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "What was going on?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# EXAMPLE: BUILD A **ONE-SHOT** PROMPT\n",
        "#   • “One-shot” = give the model ONE demo pair (dialogue + summary)\n",
        "#     before asking it to summarise a new dialogue by itself.\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Row numbers for our *demonstration* set.\n",
        "# Here we choose row 40 (any single row works).\n",
        "example_indices_full = [40]           # ← 1 demo dialogue + its summary\n",
        "\n",
        "# Row number the model must now summarise (no answer provided).\n",
        "example_index_to_summarize = 200      # ← target dialogue\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Call the helper we wrote earlier to assemble the big prompt:\n",
        "#   [Dialogue 40 + Q + Summary]\n",
        "#   <blank line>\n",
        "#   [Dialogue 200 + Q]    ← model will continue from here\n",
        "# ------------------------------------------------------------\n",
        "one_shot_prompt = make_prompt(example_indices_full,  # demo rows\n",
        "                              example_index_to_summarize # unseen row\n",
        "                              )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Inspect the resulting prompt text so you can verify the format\n",
        "# before sending it through the model.\n",
        "# ------------------------------------------------------------\n",
        "print(one_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "awFePvHnXeYH"
      },
      "source": [
        "Now pass this prompt to perform the one shot inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vuHd1-WpXeYH",
        "outputId": "12ce8941-9833-4384-8acc-655d7664dc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ONE SHOT:\n",
            "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1) GRAB THE GOLD-STANDARD SUMMARY\n",
        "#    • We already picked example_index_to_summarize = 200.\n",
        "#    • Pull the human-written summary for that row so we can\n",
        "#      compare the model’s one-shot output to the reference.\n",
        "# ------------------------------------------------------------\n",
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) TOKENISE THE ONE-SHOT PROMPT AND GENERATE A SUMMARY\n",
        "#    • one_shot_prompt  was built in the previous cell:\n",
        "#         [demo dialogue + answer]  +  target dialogue + question\n",
        "#    • model.generate(...)\n",
        "#         – reads the full prompt (demo + new dialogue)\n",
        "#         – continues writing after the final question\n",
        "#    • max_new_tokens=50  caps the length of the model’s answer\n",
        "# ------------------------------------------------------------\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) SHOW BASELINE VS. MODEL OUTPUT SIDE-BY-SIDE\n",
        "# ------------------------------------------------------------\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "eKDdt-vEXeYI"
      },
      "source": [
        "<a name='4.2'></a>\n",
        "### 4.2 - Few Shot Inference\n",
        "\n",
        "Let's explore few shot inference by adding two more full dialogue-summary pairs to your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "D0ku14iKXeYI",
        "outputId": "ec2895bc-b32b-4e14-f97e-7367999b1250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "What was going on?\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "What was going on?\n",
            "Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
            "#Person2#: Yes. Thank you very much. \n",
            "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
            "#Person2#: Oh, is it? \n",
            "#Person1#: Would you change it to a new one? \n",
            "#Person2#: Yes, certainly. You have the receipt? \n",
            "#Person1#: Yes, I do. \n",
            "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
            "#Person1#: Thank you so much. \n",
            "\n",
            "What was going on?\n",
            "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "What was going on?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# EXAMPLE: BUILD A **FEW-SHOT** PROMPT (3 demos + 1 target)\n",
        "#   • “Few-shot” = give the model SEVERAL demonstration pairs\n",
        "#     (dialogue + correct summary) before asking it to summarise\n",
        "#     a new dialogue by itself.\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Row numbers for *demonstration* examples (3 in this case).\n",
        "# Feel free to swap in any other valid row indices.\n",
        "example_indices_full = [40, 80, 120]      # 3 demo dialogues + summaries\n",
        "\n",
        "# Row number that the model must summarise (no answer provided).\n",
        "example_index_to_summarize = 200          # 1 target dialogue\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Call make_prompt() to assemble the big prompt:\n",
        "#      [Dialogue 40 + Q + Summary 40]\n",
        "#      [Dialogue 80 + Q + Summary 80]\n",
        "#      [Dialogue 120 + Q + Summary 120]\n",
        "#      [Dialogue 200 + Q]   <-- model will continue here\n",
        "# ------------------------------------------------------------\n",
        "few_shot_prompt = make_prompt(\n",
        "    example_indices_full,                 # demo rows\n",
        "    example_index_to_summarize            # target row\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Print the resulting prompt so you can verify the format\n",
        "#    before passing it through the model.  You should see three\n",
        "#    complete dialogue–summary pairs, followed by a *fourth*\n",
        "#    dialogue ending right after the question “What was going on?”\n",
        "# ------------------------------------------------------------\n",
        "print(few_shot_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "o9T0Q_GSXeYI"
      },
      "source": [
        "Now pass this prompt to perform a few shot inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "r2421gKXXeYI",
        "outputId": "3a9387b7-e2c3-42cb-d24c-e2a4b2512d0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - FEW SHOT:\n",
            "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# EVALUATE THE MODEL WITH **FEW-SHOT** PROMPTING\n",
        "#   • few_shot_prompt   already contains 3 demo Q-A pairs\n",
        "#     plus the target dialogue (row 200) with no answer.\n",
        "#   • We’ll run the model, then compare its output to the\n",
        "#     ground-truth human summary for row 200.\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Fetch the *reference* (gold-standard) summary for row 200\n",
        "#    so we have something to compare the model’s answer against.\n",
        "# ------------------------------------------------------------\n",
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) TOKENISE the few-shot prompt and let the model GENERATE\n",
        "#    up to 50 new tokens (its predicted summary).\n",
        "# ------------------------------------------------------------\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt') # text → IDs → tensors\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"], # encoded prompt (demo + target dialogue)\n",
        "        max_new_tokens=50, # cut off after 50 generated tokens\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Print baseline vs. model output side-by-side for inspection\n",
        "# ------------------------------------------------------------\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "IKHNtNT_XeYI"
      },
      "source": [
        "In this case, few shot did not provide much of an improvement over one shot inference.  And, anything above 5 or 6 shot will typically not help much, either.  Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens.  Anything above the context length will be ignored.\n",
        "\n",
        "However, you can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "hwfdktoLXeYI"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "Experiment with the few shot inferencing.\n",
        "- Choose different dialogues - change the indices in the `example_indices_full` list and `example_index_to_summarize` value.\n",
        "- Change the number of shots. Be sure to stay within the model's 512 context length, however.\n",
        "\n",
        "How well does few shot inferencing work with other examples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9AS4m3qMXeYI"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Generative Configuration Parameters for Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "h7rX0__nXeYJ"
      },
      "source": [
        "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).\n",
        "\n",
        "A convenient way of organizing the configuration parameters is to use `GenerationConfig` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "0PEt57LKXeYJ"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "Change the configuration parameters to investigate their influence on the output.\n",
        "\n",
        "Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`).\n",
        "\n",
        "Uncomment the lines in the cell below and rerun the code. Try to analyze the results. You can read some comments below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gzKqtJw7XeYJ",
        "outputId": "63af6396-9db8-4037-f0fa-867cdc75c466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - FEW SHOT:\n",
            "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# PLAYING WITH GENERATION SETTINGS\n",
        "#\n",
        "# Hugging Face provides a handy “GenerationConfig” object that bundles\n",
        "# all decoding knobs (max tokens, sampling vs. greedy, temperature…).\n",
        "# You create the config once and pass it into model.generate().\n",
        "#\n",
        "# Below are **five** example configs:\n",
        "#   1) 50-token cap, greedy decoding          (deterministic)\n",
        "#   2) 10-token cap, greedy (short summary)   (commented out)\n",
        "#   3) 50-token cap, sampling  T=0.1  (very cautious / low diversity)\n",
        "#   4) 50-token cap, sampling  T=0.5  (balanced creative/factual)\n",
        "#   5) 50-token cap, sampling  T=1.0  (high diversity, risk of drift)\n",
        "#\n",
        "# Uncomment the one you want to test.\n",
        "# ------------------------------------------------------------------\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(max_new_tokens=50)\n",
        "# generation_config = GenerationConfig(max_new_tokens=10)                       # short & terse\n",
        "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
        "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
        "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1) Encode the few-shot prompt (demo Q-A pairs + target dialogue)\n",
        "# ------------------------------------------------------------------\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2) Generate the model’s summary using the chosen config\n",
        "#    • model.generate(..., generation_config=…) reads every field\n",
        "#      from the config (max tokens, sampling flags, temperature, etc.)\n",
        "# ------------------------------------------------------------------\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        generation_config=generation_config,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4) Display the model’s few-shot summary and the gold reference\n",
        "# ------------------------------------------------------------------\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnhAJiiDXeYJ"
      },
      "source": [
        "Comments related to the choice of the parameters in the code cell above:\n",
        "- Choosing `max_new_tokens=10` will make the output text too short, so the dialogue summary will be cut.\n",
        "- Putting `do_sample = True` and changing the temperature value you get more flexibility in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEarbB3nXeYJ"
      },
      "source": [
        "As you can see, prompt engineering can take you a long way for this use case, but there are some limitations. Next, you will start to explore how you can use fine-tuning to help your LLM to understand a particular use case in better depth!"
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}